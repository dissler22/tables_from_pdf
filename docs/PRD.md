# PRD ‚Äì Extracteur de tableaux PDF bas√© sur GMFT

## Short brief ‚Äî But & fonctionnement (vue d'ensemble)
- **But** : proposer une solution souveraine permettant de d√©tecter, structurer et exporter automatiquement tous les tableaux pr√©sents dans des PDF h√©t√©rog√®nes gr√¢ce √† GMFT (Graph-based Multi-layout Table Finder) et √† une cha√Æne Python industrialisable.
- **Fonctionnement r√©sum√©** :
  1. **Ingestion** : les fichiers sont transf√©r√©s via l'API `/pdf/upload`, la CLI `gmft-cli ingest` ou l'interface Streamlit. Les lots volumineux utilisent les sessions (`/pdf/upload-sessions/*`) pour d√©coupler l'upload du traitement. Chaque page est normalis√©e (rotation, contraste, split recto-verso) avant d'√™tre encapsul√©e pour GMFT.
  2. **Extraction** : GMFT identifie les zones tabulaires (multi-table/multi-colonnes), reconstruit les grilles puis convertit les cellules en DataFrame pandas. Les textes issus d'OCR (Tesseract ou PaddleOCR) sont fusionn√©s avec les PDF natifs afin d'obtenir des cellules propres et align√©es.
  3. **Publication** : les DataFrame enrichis sont stock√©s dans `tables_store/` (JSON + artefacts Parquet). Le moteur expose des exports CSV/Parquet/Excel et un sch√©ma Arrow pour la connexion aux pipelines de data-engineering.
  4. **Consommation** : une API Python (`gmft_extractor.client`), une CLI robuste et les routes REST `/tables/query`, `/tables/{table_id}` et `/pipelines/hooks` permettent d'orchestrer les extractions, suivre les jobs et brancher des heuristiques m√©tier.

## Synth√®se statuts (TDD)
| Feature | Description | Statut |
| --- | --- | --- |
| Feature 1.1 | Ingestion multi-origines (API, CLI, UI) | ‚úÖ |
| Feature 1.2 | D√©tection de tableaux GMFT + OCR hybride | ‚úÖ |
| Feature 1.3 | Normalisation/standardisation des sch√©mas | ‚úÖ |
| Feature 1.4 | Indexation, exports multiples et requ√™tes | ‚úÖ |
| Feature 1.5 | API REST & SDK Python | ‚úÖ |
| Feature 1.6 | Observabilit√© (logs, m√©triques, traces) | ‚úÖ |
| Feature 1.7 | Outils d'ex√©cution & tests automatis√©s | ‚úÖ |
| Feature 2.0 | Principes UX g√©n√©raux (pages Streamlit) | ‚úÖ |
| Feature 2.2 | Page ¬´ Recherche ¬ª (visualisation tables) | ‚úÖ |
| Feature 2.3 | UX & accessibilit√© (filtres, dataviz) | ‚úÖ |
| Feature 2.4 | Page ¬´ Gestion des heuristiques ¬ª | üöß |
| Feature 3.1 | Infra minimale (Backend FastAPI + Front) | ‚úÖ |
| Feature 3.2 | S√©curit√© & gouvernance des donn√©es | ‚úÖ |
| Feature 3.3 | Points d'attention (docs, r√©silience GMFT, extension m√©tier) | ‚úÖ |

---

## Partie 1 ‚Äî Pipeline, donn√©es et algorithmes

### 1) Ingestion des documents
- **Sources pr√©vues** : PDF natifs ou scann√©s d√©pos√©s dans `tests/data/pdf_tables/`, uploads utilisateurs (Streamlit, API `multipart/form-data`), archives ZIP trait√©es c√¥t√© serveur (`/pdf/upload-zip`), ou `gmft-cli ingest --watch <folder>` pour surveiller un r√©pertoire.
- **Normalisation des chemins** : chaque d√©p√¥t g√©n√®re un `table-run-<hash>` ; les fichiers sources sont recopi√©s dans `data/uploads/` et horodat√©s. Le nom original est stock√© dans `doc_status` et sur tous les artefacts (`_source_name`).
- **Tracking du traitement** : l'API retourne un `run_id`. La route `/pdf/track_status/{run_id}` expose le statut global, le nombre de pages analys√©es, le nombre de tableaux extraits et les erreurs GMFT/OCR (`WAITING`, `PREPROCESSING`, `DETECTING`, `POSTPROCESSING`, `DONE`, `FAILED`).
- **Sessions d'upload longues** : `/pdf/upload-sessions` cr√©e un conteneur temporaire (`data/uploads/__sessions__/<session_id>`) dans lequel les utilisateurs poussent des PDF ou ZIP. La commande `/commit` d√©clenche l'ordonnanceur `gmft_worker`. Les fichiers sont purg√©s apr√®s confirmation pour limiter la surface disque.
- **R√©pertoires runtime** :
  - `data/metadata/` : m√©tadonn√©es des runs (pages, orientation, heuristiques appliqu√©es, mapping colonnes).
  - `data/tables_store/` : tables brutes (par page) + agr√©gations multi-pages (JSON/Parquet/Feather).
  - `data/layouts/` : masques GMFT (`*.gmft.json`) et images annot√©es (`*_preview.png`) accessibles via `/tables/{table_id}/preview`.

### 2) OCR & extraction de m√©tadonn√©es
- **Technologie OCR** : GMFT consomme directement le texte natif lorsqu'il existe ; sinon `pytesseract` (par d√©faut) ou PaddleOCR (`OCR_ENGINE=paddle`) reconstitue les cellules. Le module `backend/gmft_core/ocr/ocr_router.py` g√®re les retries (`tenacity`) et la fusion multi-langues.
- **Extraction GMFT** : `backend/gmft_core/pipeline/gmft_runner.py` orchestre GMFT (https://github.com/conjuncts/gmft) pour d√©tecter les r√©gions, vectoriser les lignes/colonnes puis produire une grille ordonn√©e. Chaque cellule est accompagn√©e d'un `bbox`, d'un `confidence` et d'un `source_layer` (texte natif vs OCR).
- **Enrichissement tabulaire** :
  - D√©tection des en-t√™tes r√©currents (`headline_detector.py`) pour remplir `schema.columns`.
  - Fusion des cellules fusionn√©es (row/col span) en pr√©servant l'ordre de lecture.
  - Alignement sur `table_metadata.json` : type de donn√©es, unit√©s, indice m√©tier associ√©.
- **D√©tection langue & localisation** : `langdetect` + `pycountry` alimentent `table_language` et `number_format` afin d'interpr√©ter correctement les s√©parateurs d√©cimaux.
- **Propagation** : toutes les m√©tadonn√©es pertinentes (dossier, provenance, heuristiques utilis√©es, statut OCR) sont copi√©es dans les DataFrame export√©s (`df.attrs`) et dans les chunks Arrow pour √™tre filtrables c√¥t√© requ√™tes.
- **Rechargement partiel** : `backend/gmft_core/scripts/replay_run.py` relance le parsing d'un `run_id` en r√©utilisant les pages normalis√©es (`--from-cache`) ou en recalculant uniquement les heuristiques m√©tier (`--features-only`). La page Ingestion du front offre un bouton ¬´ Rejouer avec un nouveau profil ¬ª qui appelle ce script via `gmft_ops.replay_run`.

### 3) Canonisation & r√©f√©rentiels
- **Organisations (`data/refs/refs_user.json`)** : r√©utilis√©es pour mapper les dossiers logiques (service m√©tier, client, march√©). Elles servent √† pr√©-remplir `folder` et √† taguer les exports, ce qui facilite l'automatisation downstream.
- **Personnes / heuristiques (`data/refs/refs_people.json`)** : dans ce projet, le fichier stocke d√©sormais les **profils d'extraction** : auteur, scope, alias et listes de colonnes attendues. L'API `/people` devient `/profiles` mais conserve les m√™mes contrats CRUD afin de ne pas casser les int√©grations existantes.
- **Maintenance** : `backend/gmft_core/scripts/update_refs.py` synchronise les profils locaux et ceux d√©finis en API. Les administrateurs peuvent charger un YAML m√©tier (heurs de facturation, identifiants de lignes budg√©taires) puis propager les alias via ce script ou via la page ¬´ Gestion des heuristiques ¬ª (voir Partie 2).

### 4) Indexation & recherche
- **Table store** : chaque tableau est stock√© dans `tables_store/kv_store_tables.json` avec son `table_id`, la page d'origine, les colonnes d√©tect√©es, les statistiques (`row_count`, `column_count`, `measurements`). Les exports Parquet sont g√©n√©r√©s dans `tables_store/parquet/<table_id>.parquet` pour √™tre directement consomm√©s par Spark ou pandas.
- **Query engine** : `backend/gmft_core/query/table_query.py` propose deux modes :
  - `filtered` (par d√©faut) qui applique un filtrage dur sur `folder`, `source_file`, `column_set`, `language`, `confidence_min`.
  - `layout` qui exploite les graphes GMFT (`graph_table_relation.graphml`) pour retrouver des tables similaires (structure + densit√©), utile pour d√©tecter des rapports identiques sur plusieurs mois.
- **Exports** : `/tables/{table_id}/export` accepte `format=csv|parquet|xlsx` et renvoie un flux streamable. Chaque export contient les m√©tadonn√©es dans un onglet/feuille `__meta__` (Excel) ou dans `pandas.DataFrame.attrs`.
- **Recherche textuelle** : `tables_search_index.sqlite` indexe les cellules pour permettre des recherches full-text rapides c√¥t√© CLI (`gmft-cli query --text "montant TVA"`).

### 5) API REST principale (FastAPI)
- D√©marrage via `python -m gmft_core.api.server` (cf. `docs/other/QUICKSTART.md`).
- **Routes documents** (`backend/gmft_core/api/routers/pdf_routes.py`) :
  - `POST /pdf/upload` : upload unique avec param√®tres `folder`, `profile_id`, `extract_strategy` (auto / force_gmft / force_ocr).
  - `POST /pdf/upload-zip` : import massif ; g√®re jusqu'√† 500 Mo avec streaming disque.
  - `POST /pdf/text` : injection d'un export texte externe (pour tests OCR).
  - `GET /pdf/track_status/{run_id}` : statut d√©taill√© par page.
  - `GET /pdf/folders` : liste maintenue via r√©f√©rentiels.
  - `POST /pdf/list` : vue consolid√©e des runs (filtrable par profil, statut, p√©riode).
  - `GET /pdf/{doc_id}/download` : acc√®s au PDF d'origine (contr√¥le d'acc√®s par namespace).
- **Routes tables** (`backend/gmft_core/api/routers/table_routes.py`) :
  - `POST /tables/query` : filtrage/agr√©gation (mode `filtered` ou `layout`).
  - `GET /tables/{table_id}` : m√©tadonn√©es, aper√ßu PNG, statue heuristiques appliqu√©es.
  - `GET /tables/{table_id}/export` : export multi-format.
- **Routes r√©f√©rentiels & hooks** :
  - `/references` pour g√©rer les alias dossiers.
  - `/profiles` (h√©ritage `/people`) pour stocker les profils heuristiques.
  - `/hooks` pour d√©clarer des modules d'extension (voir ¬ß3 Points d'attention).
- **Auth** : d√©pendances `gmft_core.auth.get_auth` ; support cl√© API (`X-API-Key`), namespaces (`X-Workspace`) et OAuth interne (optionnel) pour la CLI.
- **Ergonomie** : `frontend/streamlit/services/api_client.py` mutualise les appels, g√®re l'attente des jobs longs via polling exponentiel et renvoie des DataFrame pandas directement exploitables par la page Recherche.

### 6) Observabilit√© & logs
- **Logs structur√©s** : `gmft_core.utils.logger` produit des √©v√©nements JSON (√©tape, dur√©e, nb de tables). Les diagnostics (`scripts/diagnostics/check_upload.py`, `scripts/diagnostics/check_table_quality.py`) lisent ces fichiers pour rep√©rer les anomalies GMFT.
- **M√©triques** : `prometheus_fastapi_instrumentator` expose `gmft_tables_detected_total`, `gmft_run_duration_seconds`, `gmft_export_errors_total`. Un dashboard Grafana mod√®le est fourni dans `observability/grafana/gmft_tables.json`.
- **Suivi pipeline** : `run_status.json` garde l'historique complet (m√™me apr√®s purge) afin que la page Ingestion puisse afficher les tendances (dur√©es moyennes, taux d'erreur OCR).

### 7) Ex√©cution & tests
- **Scripts CLI** :
  - `backend/gmft_core/scripts/ingest_sample.py` : pipeline complet sur `tests/data/pdf_tables/`.
  - `gmft-cli query` et `gmft-cli export` : sc√©narios de bout en bout utilis√©s dans `docs/other/QUICKSTART.md`.
  - `scripts/diagnostics/check_profile.py` : valide les profils heuristiques.
- **Tests automatis√©s** :
  - `tests/backend/test_table_api.py` : CRUD sur `/tables/*` et `/profiles`.
  - `tests/integration/test_cli_ingest.py` : v√©rifie ingestion + export CSV.
- **Environnements requis** :
  - `backend/gmft_core/pyproject.toml` + `backend/gmft_core/requirements-ocr.txt` (pour Paddle/Tesseract) + `frontend/streamlit/requirements.txt`.
  - Variables document√©es dans `CONFIG.md` (backend) et `FRONTEND_CONFIG_GUIDE.md` (front) ; `GMFT_MODEL_PATH` et `GMFT_CACHE_DIR` sont obligatoires.

---

## Partie 2 ‚Äî Front Streamlit (comportement & UX)

### 0) Principes g√©n√©raux
- Application Streamlit unique (`frontend/streamlit/app.py`) avec trois pages : `1_Ingestion.py`, `2_Recherche.py`, `3_Gestion_heuristiques.py`. Navigation via menu lat√©ral ; √©tat commun (`st.session_state`) pour partager les derniers runs et profils charg√©s.
- **Configuration dynamique** : `frontend/streamlit/utils/config.py` lit `.env` et expose `API_BASE_URL`, `API_KEY`, `DEFAULT_PROFILE`. Les valeurs sont affich√©es dans un encart diagnostic pour aider les √©quipes data.
- **API Client** : `frontend/streamlit/services/api_client.py` encapsule toutes les routes (upload, status, tables, exports, profils) et traduit les r√©ponses JSON en DataFrame.
- **Stockage d'√©tat** : `session_state` garde les dossiers, profils s√©lectionn√©s, derniers exports et pr√©f√©rences d'affichage (format large / condens√©).

### 1) Page ¬´ Ingestion ¬ª
- **Objectifs** : permettre aux √©quipes m√©tiers de lancer des extractions et de comparer la qualit√© GMFT vs OCR.
- **Fonctionnalit√©s cl√©s** :
  - Upload drag & drop multi-fichiers avec s√©lection du profil heuristique et du dossier logique.
  - Onglet ZIP (import mensuel) + champ pour s√©lectionner une session existante.
  - R√©sum√© temps r√©el : nb de pages trait√©es, nb de tables par document, alertes (taux de confiance < seuil, colonnes manquantes).
  - Tableau des runs r√©cents avec boutons ¬´ Rejouer ¬ª, ¬´ Exporter toutes les tables ¬ª, ¬´ T√©l√©charger les pr√©views ¬ª.
  - Panneau ¬´ ‚ôªÔ∏è Recalcul ¬ª : choix du mode (rejouer complet, recharger depuis cache, heuristiques uniquement) qui d√©clenche `gmft_ops.replay_run` et affiche stdout/stderr dans un `st.expander`.
  - Gestion des erreurs : messages explicites si GMFT indisponible ou si un OCR tiers manque ; suggestions automatiques (installer `tesseract`, v√©rifier `GMFT_MODEL_PATH`).

### 2) Page ¬´ Recherche ¬ª
- **Objectifs** : filtrer et visualiser les tableaux extraits, puis exporter ceux qui int√©ressent l'utilisateur.
- **Filtres disponibles** : dossier, profil, statut de run, date (p√©riode), langue, mot-cl√© dans les colonnes, plage du nombre de colonnes. Les filtres sont stock√©s c√¥t√© session pour faciliter le va-et-vient avec la page Ingestion.
- **R√©sultats** :
  - Tableau interactif (`st.dataframe`) listant `table_id`, run, page, nb lignes, colonnes d√©tect√©es (chips), niveau de confiance.
  - Boutons d'action : pr√©visualiser (ouvre l'image annot√©e), t√©l√©charger CSV/Parquet/Excel, ouvrir dans pandas (affichage inline via `st.data_editor`).
  - Statistiques : totaux par profil, moyenne de lignes, ratio OCR vs texte natif.
- **Recherche RAG-like** : champ libre alimentant `/tables/query` mode `layout` pour retrouver des tables structurellement similaires (ex : "tableaux de d√©penses mensuelles"). R√©sultats pr√©sent√©s avec un score, l'origine et une recommandation d'export.

### 3) Page ¬´ Gestion des acteurs ¬ª
- **Objectifs** : piloter les profils heuristiques (alias colonnes, r√®gles de nettoyage) et les dossiers logiques depuis Streamlit.
- **Vue profils** :
  - Filtres : dossier + type de profil (global, local, exp√©rimental).
  - Tableau `st.dataframe` listant nom, alias, colonnes attendues, modules d'extension activ√©s.
  - Formulaire d'ajout/mise √† jour (`APIClient.upsert_profile`) avec options `merge_columns` et `append_hooks`.
  - Actions rapides : dupliquer un profil, exporter/importer en JSON, forcer une synchronisation avec le backend.
- **Bloc dossiers** :
  - Liste des dossiers disponibles (`/references/folders`).
  - Boutons ¬´ Copier ¬ª, ¬´ Nettoyer ¬ª, ¬´ Recharger depuis backend ¬ª pour maintenir les alias.
  - Historique des modifications stock√© dans `st.session_state['folders_audit']`.

### 4) UX & accessibilit√©
- Interface full FR, ic√¥nes üéØüìÑüìä pour rythmer les cards.
- Rappels contextuels : infobulles pour expliquer `confidence`, `profil`, `layout score`.
- Mode compact pour afficher >50 tables ; switch accessible via la barre lat√©rale.
- Guide de configuration (`FRONTEND_CONFIG_GUIDE.md`) mis √† jour avec les variables sp√©cifiques GMFT (profils par d√©faut, dossier d'exports partag√©s).

---

## Partie 3 ‚Äî Exigences techniques & contraintes

### 1) Infrastructure minimale
- **Backend** : Python 3.10+, GMFT (sous-module Git) + PyMuPDF, pdfplumber, pytesseract, PaddleOCR (optionnel). Variables obligatoires : `GMFT_MODEL_PATH`, `GMFT_CACHE_DIR`, `TESSDATA_PREFIX`.
- **Frontend** : Streamlit ‚â•1.51, Plotly pour les pr√©views et `streamlit-aggrid` pour la data grid.
- **Stockage** : 200 Mo/run pour les pr√©views + Parquet ; pr√©voir 5 Go libres pour des projets moyens. Les exports sont compress√©s via `pyarrow`.
- **Performances** : objectif <45 s pour un PDF de 50 pages en mode mixte (texte + OCR). Les exports sont stream√©s pour √©viter les timeouts.

### 2) S√©curit√© & conformit√©
- **Secrets** : cl√©s OCR (si usage cloud) stock√©es serveur-side (Vault ou `.env`).
- **Gouvernance** : les PDF sont isol√©s par dossier logique (`namespace`). Les exports peuvent √™tre chiffr√©s (`--encrypt`).
- **Tra√ßabilit√©** : chaque table garde un hash du PDF d'origine (`source_hash`). Les journaux listent qui a t√©l√©charg√© quoi (page Recherche > Audit).

### 3) Points d'attention
- **Documentation** : nombreuses r√©f√©rences (`INDEX.md`, `README_adaptations.md`, `PROJECT_STRUCTURE.md`). Les pages GMFT doivent rester synchronis√©es : tout changement de script ‚Üí mise √† jour doc associ√©e.
- **R√©silience GMFT** : pr√©voir des fallback (d√©tection heuristique `legacy_detector`) quand GMFT √©choue sur des scans tr√®s bruit√©s. Les alertes critiques d√©clenchent une notification via `/hooks`.
- **Extensions m√©tier** : l'√©cosyst√®me repose sur les hooks d'extension (module `backend/gmft_core/hooks`). Les √©volutions doivent documenter les invariants : format d'entr√©e DataFrame, variables d'environnement requises.
- **Compatibilit√© CLI** : `gmft-cli` doit rester compatible Windows/Linux ; les chemins g√©r√©s par `pathlib` et les exports compress√©s sous forme relative.

---

## Glossaire
- **GMFT** : librairie open source (Graph-based Multi-layout Table Finder) permettant de d√©tecter et de structurer les tableaux dans des PDF complexes.
- **Profil** : configuration regroupant les heuristiques m√©tier (liste de colonnes attendues, r√®gles de nettoyage, hooks).
- **Run** : traitement complet d'une ou plusieurs sources PDF ; contient les pages normalis√©es et les tables produites.
- **Table ID** : identifiant stable d'un tableau, utilis√© pour tout export/filtrage.
- **Hooks** : modules Python inject√©s dans la pipeline pour enrichir ou transformer les DataFrame g√©n√©r√©es.

---

## Ouvertures (post-v1)
- Support d'autres moteurs (Detectron2, TabStructNet) et d'exports SQL directs.
- Normalisation automatique des unit√©s (monnaies, pourcentages, quantit√©s) via un moteur de r√®gles.
- Visualisations interactives temps r√©el (graphiques deriv√©s des tables) directement dans Streamlit.
- Autoscaling du worker GMFT (Ray ou Celery) pour traiter des milliers de PDF en parall√®le.

---

## Organisation du d√©veloppement (r√©f√©rence TDD incr√©mentale)
- **D√©coupage** : chaque incr√©ment couvre un maillon du pipeline (pr√©traitement, d√©tection, normalisation, export). Les d√©pendances sont rappel√©es dans la fiche feature concern√©e.
- **Cadre TDD** : √©crire d'abord les tests CLI/API (ex. `pytest tests/backend/test_table_api.py -k export`), ensuite impl√©menter, puis factoriser. Conserver des fixtures PDF synth√©tiques dans `tests/data/pdf_tables/`.
- **Suivi documentaire** : mettre √† jour cette PRD et les fiches `docs/features/<feature>.md` lorsqu'une feature bascule de üöß √† ‚úÖ. Mentionner les commandes exactes permettant de retester.
- **Tests** : un test repr√©sentatif par feature (ingestion, d√©tection, export, hooks) consign√© dans `docs/TEST_REGISTRY.md`.
- **R√©f√©rence** : le prompt TDD complet (`specs/tdd/DEV_PROMPT_REFERENCE.md`) reste la source pour alimenter Codex.
- **Commandes** : privil√©gier `pytest`, `gmft-cli` et les scripts list√©s dans `docs/runbooks/COMMANDS.md`. Toute nouvelle commande doit √™tre ajout√©e √† ce runbook et li√©e √† la feature.
- **Commits** : cycle `test/feat/refactor` par incr√©ment, validation obligatoire des tests critiques avant merge.
